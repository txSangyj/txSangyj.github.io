<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://txSangyj.github.io/</id>
    <title>ji</title>
    <updated>2023-01-07T04:08:41.877Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://txSangyj.github.io/"/>
    <link rel="self" href="https://txSangyj.github.io/atom.xml"/>
    <subtitle>学习用</subtitle>
    <logo>https://txSangyj.github.io/images/avatar.png</logo>
    <icon>https://txSangyj.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, ji</rights>
    <entry>
        <title type="html"><![CDATA[ 2022书单]]></title>
        <id>https://txSangyj.github.io/post/2022-shu-dan/</id>
        <link href="https://txSangyj.github.io/post/2022-shu-dan/">
        </link>
        <updated>2023-01-03T11:16:01.000Z</updated>
        <summary type="html"><![CDATA[<p>今年的总结早多了。</p>
]]></summary>
        <content type="html"><![CDATA[<p>今年的总结早多了。</p>
<!-- more -->
<h1 id="已">已</h1>
<h2 id="路边野餐">《路边野餐》</h2>
<pre><code>当时野餐露营挺火。
</code></pre>
<p>苏联时代的科幻小说，人名过于拗口了。确实几个人像工蚁一样。附录还有作者对主编修改意见的吐槽，笑死。</p>
<h2 id="神们自己">《神们自己》</h2>
<p>赶上了隔离的尾巴，在酒店隔离时候读的。</p>
<pre><code>拉蒙特发现了电子通道存在的问题，想关闭电子通道，但关闭意味倒退。
杜阿文明关系文明存亡，更不可能关闭。
狄尼特不光指出了问题，也提出了解决问题的方法。
</code></pre>
<p>拉蒙特和政治家的对话。</p>
<blockquote>
<p>“年轻人，我的权力从名义上来说很大，但是我只能在符合公众愿望的情况下才拥有这么大的权力。”</p>
</blockquote>
<h2 id="教父-parti">《教父 PART:Ⅰ》</h2>
<p>文字细节多了点，但电影确实太还原了。<br>
图书馆只有Ⅰ和Ⅲ，读完Ⅰ就没有然后了。</p>
<h2 id="兄弟">《兄弟》</h2>
<p>看了余华和罗翔的对谈去看的。<br>
上半部为宋凡平难受流了不知道多少泪，下半部看刘镇众人变化，气愤于宋钢的随波逐流。<br>
偷别人的书评：</p>
<blockquote>
<p>上半部真实的荒诞<br>
下半部荒诞的真实</p>
</blockquote>
<h2 id="皮囊">《皮囊》</h2>
<p>写作者身边的人，散文印象不深。<br>
父亲的瘫痪 倔强 偏执 精神和肉体的错位<br>
母亲的坚毅 执念 走后的思念<br>
神婆 小小</p>
<h2 id="两篇网文">两篇网文</h2>
<h3 id="深空之流浪舰队">《深空之流浪舰队》</h3>
<p>实在是太长了，跳过不少段落，只有一句话：</p>
<blockquote>
<p>愿不断进步。</p>
</blockquote>
<h3 id="我们生活在南京">《我们生活在南京》</h3>
<p>铺垫转折不断，篇幅不长好评。梗太多偶尔出戏但读起来相对轻松，<s>通信课外读物</s>。</p>
<h1 id="正">正</h1>
<h2 id="树犹如此">《树犹如此》</h2>
<h2 id="少年pi的奇幻漂流">《少年Pi的奇幻漂流》</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一个简单的脚本]]></title>
        <id>https://txSangyj.github.io/post/a/</id>
        <link href="https://txSangyj.github.io/post/a/">
        </link>
        <updated>2021-04-16T07:37:15.000Z</updated>
        <summary type="html"><![CDATA[<p>迫于被远程桌面输命令疯狂重复按键折磨吐了写的，不过这样自动补全没了。<s>好在不需要再用了</s></p>
]]></summary>
        <content type="html"><![CDATA[<p>迫于被远程桌面输命令疯狂重复按键折磨吐了写的，不过这样自动补全没了。<s>好在不需要再用了</s></p>
<!-- more -->
<pre><code class="language-python"># test.py
import win32gui,win32api,win32con
import time

def send_to_cmd():
    wndtitle = u&quot;&quot; # 窗口名称，可以spy++获取
    wndclass = None 
    wnd = win32gui.FindWindow(wndclass, wndtitle)
    win32api.keybd_event(13, 0, 0, 0)
    win32gui.SetForegroundWindow(wnd)
    win32api.SetCursorPos((1000, 200)) # 鼠标位置1
    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTDOWN, 100, 100, 0, 0)
    time.sleep(0.2)
    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTUP, 100, 100, 0, 0)
    
    wndtitle = u&quot;C:\WINDOWS\system32\cmd.exe - python  test.py&quot;
    wndclass = None 
    wnd = win32gui.FindWindow(wndclass, wndtitle)
    win32api.keybd_event(13, 0, 0, 0) #
    win32gui.SetForegroundWindow(wnd)
    win32api.SetCursorPos((100, 100)) # 鼠标位置2
    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN, 100, 100, 0, 0)
    time.sleep(0.2)
    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP, 100, 100, 0, 0)
    
    
def inputtxt(string):
    import win32clipboard as w
    w.OpenClipboard()
    w.SetClipboardData(win32con.CF_UNICODETEXT,string)
    w.CloseClipboard()

if __name__=='__main__':
    while True:
        s = input()
        if s!='quitcmd':
            inputtxt(s+'\n')
            send_to_cmd()
        else:
            break
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[对话分类竞赛总结]]></title>
        <id>https://txSangyj.github.io/post/dui-hua-fen-lei-jing-sai-zong-jie/</id>
        <link href="https://txSangyj.github.io/post/dui-hua-fen-lei-jing-sai-zong-jie/">
        </link>
        <updated>2021-03-05T01:33:52.000Z</updated>
        <summary type="html"><![CDATA[<p>一篇旧文。</p>
]]></summary>
        <content type="html"><![CDATA[<p>一篇旧文。</p>
<!-- more -->
<h1 id="总结">总结</h1>
<h2 id="初赛">初赛</h2>
<p>第一次接触NLP问题，初赛时间较为充裕，而Bert处理短文本性能足够。应尽量尝试多种可行方式解决问题，而不是只拿着Bert跑无意义的结果。</p>
<h2 id="复赛决赛">复赛&amp;决赛</h2>
<ul>
<li>复赛阶段数据量大，试错成本更高，完全应该进行大的改动再重新迭代。</li>
<li>数据处理和基础模型没有大问题，但是完全忘记了在提交结果阶段使用<strong>集成学习</strong>方法。</li>
<li>没有对长文本进行进一步处理，取首尾截断不如训练两个模型<strong>分别处理首尾</strong>之后再进行投票。</li>
<li>程序总是有Bug，一个on_epoch_end的问题就是九个小时左右时间的浪费。</li>
<li>batch_size/model 过大，训练时无法测试</li>
<li>虽然做了客服标记的统一化，但是标记本身是否正确未可知。</li>
<li>复赛阶段工作量预估不足。</li>
</ul>
<h2 id="其他">其他</h2>
<ul>
<li>文本是机器翻译得来的， 可以进一步处理。（将方言转化得到的无意义的同音词替换为行业高频用词是一个好方向）</li>
<li>或者将对话双方文本分别训练也是一个方向。</li>
<li>长文本甚至可以做一个摘要生成的模型来处理成短文本。</li>
<li>BiLSTM+Attention机制没有做完，或者在BiLSTM各时间步上隐层上取Max做输出，在其他数据集上效果均优于直接使用BiLSTM。</li>
<li>梯度累加变相增大Batch_size的实验没有做完，新建Module类可以解决loss和原计算图无法关联的问题。（较小的模型比如共享参数的TextCNN可以做到256的batch_size，16倍于bert方法）</li>
<li>Bert本身没有问题，但是训练用的语料不同效果不同，也可能影响最后效果。</li>
</ul>
<h2 id="后续应用">后续应用</h2>
<ul>
<li>
<p>落地方面思考不足，自然语言处理工作在客服对话上可用的有：</p>
<ol>
<li>投诉分类（将和故障高度相关的投诉分类抽取，设置告警（一个问题是误告警怎么闭环处理，没有进行一个较好的回答）</li>
<li>多分类，客户可能存在多种需求，单个标签描述不便（程序上改为输出层sigmoid激活，使用二元分类损失，按每类概率&gt;0.5取对应标签。</li>
<li>不局限于分类，语义理解聚类客户诉求、根据对话方式进行用户分类匹配客服、为什么会出现长对话（话术提炼、优化效率）。</li>
</ol>
</li>
<li>
<p>TensorRT推理速度有极大提高（成百倍500ms-&gt;2ms），可以尝试。</p>
</li>
</ul>
<hr>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2021书单]]></title>
        <id>https://txSangyj.github.io/post/2021-shu-dan/</id>
        <link href="https://txSangyj.github.io/post/2021-shu-dan/">
        </link>
        <updated>2021-03-03T14:53:31.000Z</updated>
        <summary type="html"><![CDATA[<p>都三月了，书看了几本？<br>
2022.3.7 Update 回顾一下。</p>
]]></summary>
        <content type="html"><![CDATA[<p>都三月了，书看了几本？<br>
2022.3.7 Update 回顾一下。</p>
<!-- more -->
<h2 id="已">已：</h2>
<h3 id="娱乐至死">《娱乐至死》</h3>
<p>像看一篇论文。不看抖音，看完微博都想卸了。应该控制一下获取无用信息的频率了。<br>
回想起来私货很多。</p>
<h3 id="小狗钱钱12">《小狗钱钱》1&amp;2</h3>
<pre><code>    1. 小目标，开家小书店要多少钱？
    2. 养鹅🐧
    3. “这真的有必要吗”
    4. 周四记账
    5. 第二册更像“儿童读物”
</code></pre>
<h2 id="在">在：</h2>
<h3 id="伟大作品的隐秘结构">《伟大作品的隐秘结构》</h3>
<p>无结论的两难结构：举例有《伽利略传》、《老人与海》等等。<s>《我们与恶的距离》算不算...</s><br>
<s>半透明的双层结构：还没看到。。。</s></p>
<blockquote>
<p>艺术创造靠一种神奇的虚设触及了人们的两重共性：一是所刻画的对象在人们中的共性；二是欣赏者内心的某种共性。</p>
</blockquote>
<pre><code>艺术眼光不是政治眼光里提到契诃夫的《万卡》，随手一搜发现《山海情》里念的课文就是这个。
</code></pre>
<blockquote>
<p>后面发现是教材，看了个序？</p>
</blockquote>
<h3 id="沙丘"><s>《沙丘》</s></h3>
<h2 id="~~读译文真是痛苦不知道能不能坚持读完-~~果然没看完-202237-update">~~读译文真是痛苦，不知道能不能坚持读完。~~果然没看完。<br>
2022.3.7 Update</h2>
<h3 id="丧钟为谁而鸣">《丧钟为谁而鸣》</h3>
<p>2077里面带出来的。不过也没读完，进度50%吧。</p>
<h3 id="风声">《风声》</h3>
<p>回顾完电影去看，发现内容居然差别有点大。</p>
<h3 id="文化苦旅">《文化苦旅》</h3>
<p>10+年后又看王道士。</p>
<h3 id="白鹿原">《白鹿原》</h3>
<p>看完气的吃不下饭。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[aarch64下编译libtorch]]></title>
        <id>https://txSangyj.github.io/post/armv8-bian-yi-libtorch/</id>
        <link href="https://txSangyj.github.io/post/armv8-bian-yi-libtorch/">
        </link>
        <updated>2019-08-15T02:32:27.000Z</updated>
        <summary type="html"><![CDATA[<p>把写的libtorch调用模型的程序移植到了下Ubuntu下发现没有问题，libtorch提供了预编译好的库。放到基于Arm的国产主机上就提示libtorch.so不是动态库文件，需要从源代码编译libtorch。不过没啥难点，踩坑文。</p>
]]></summary>
        <content type="html"><![CDATA[<p>把写的libtorch调用模型的程序移植到了下Ubuntu下发现没有问题，libtorch提供了预编译好的库。放到基于Arm的国产主机上就提示libtorch.so不是动态库文件，需要从源代码编译libtorch。不过没啥难点，踩坑文。</p>
<!-- more -->
<h1 id="获取源码">获取源码</h1>
<pre><code class="language-bash">
git clone https://github.com/pytorch/pytorch --recursive &amp;&amp; cd pytorch
git checkout v1.2.0 # 真的勇士敢于在master分支下编译
#下载编译需要的子模块
git submodule sync
git submodule update --init --recursive
</code></pre>
<h1 id="python环境">python环境</h1>
<p>pytorch官方给出的环境是用conda装的，然而悲剧的是官方并没有给出aarch64的安装文件，所以还是使用pip来安装。还有个问题是很多python库并没有发布对应架构下的包，好在编译libtorch需要的python环境较为简单。编译完之后导出python环境发现，编译libtorch必须的包应该只有pyyaml（不太确定）。当然cmake和gcc等工具还是必须的。<br>
安装pyyaml的命令：</p>
<pre><code>pip install pyyaml
</code></pre>
<blockquote>
<p>不使用tools下的脚本而直接用cmake来编译的话可能python环境都不需要了...</p>
</blockquote>
<h1 id="编译libtorch">编译libtorch</h1>
<p>在编译之前可以使用export或者cmake-gui关闭一些不必要的编译选项来加快编译速度，类似：</p>
<pre><code>export USE_CUDA=False
export BUILD_TEST=False
</code></pre>
<p>官方提供了一键式编译工具，在pytorch/tools/build_libtorch.py。只需要运行：</p>
<pre><code class="language-bash">
#pytorch$
mkdir build &amp;&amp; cd build
python ../tools/build_libtorch.py
</code></pre>
<p>就会在build下生成对应的文件，我们需要的动态库文件libtorch.so在build/lib/下。</p>
<h1 id="整理libtorch">整理libtorch</h1>
<ol>
<li>复制pytorch下torch、caffe2、c10以及aten\source下的ATen、TH等文件夹到libtorch/include下，得到libtorch的C++接口。</li>
<li>复制build/lib下编译好的库文件到libtorch/lib下。</li>
<li>复制pytorch/torch/share/cmake文件夹到libtorch/share下。<br>
得到的libtorch主要结构：<br>
libtorch.<br>
├─include<br>
│  ├─ATen<br>
│  ├─c10<br>
│  ├─caffe2<br>
│  ├─TH<br>
│  ├─THCUNN<br>
│  ├─THNN<br>
│  └─torch<br>
├─lib<br>
└─share<br>
└─cmake<br>
├─ATen<br>
├─Caffe2<br>
├─Gloo<br>
└─Torch</li>
</ol>
<hr>
<p>如果你需要pytorch编译可以参考一下<a href="https://nmilosev.svbtle.com/compling-arm-stuff-without-an-arm-board-build-pytorch-for-the-raspberry-pi">这篇</a><br>
ps：还是对CMakeLists不太熟啊。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[torch/libtorch 多输入]]></title>
        <id>https://txSangyj.github.io/post/torchlibtorch-duo-shu-ru/</id>
        <link href="https://txSangyj.github.io/post/torchlibtorch-duo-shu-ru/">
        </link>
        <updated>2019-08-07T02:40:33.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
一个小坑，多输入的pytorch模型在导出pt文件供libtorch调用时候，python下模型的forward方法不能使用tuple的形式传入inputs。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#pytorch%E4%B8%8B">pytorch下</a>
<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5">数据输入</a></li>
</ul>
</li>
<li><a href="#libtorch%E4%B8%8B">libtorch下</a></li>
</ul>
<br>
一个小坑，多输入的pytorch模型在导出pt文件供libtorch调用时候，python下模型的forward方法不能使用tuple的形式传入inputs。</p>
<!-- more -->
<h1 id="pytorch下">pytorch下</h1>
<p>pytorch下多输入比较方便，修改模型的forward方法就可以。</p>
<h2 id="数据输入">数据输入</h2>
<p>在处理图片时经常需要多输入，比如分类时输入额外特征、检测时输入定界框，而pytorch已经实现了基本的Dataset类，实现多输入使用的就是派生一个自定义的Dataset然后实现数据读取以及__getitem__、__len__方法供Dataloader调用。</p>
<p>下面是自己用到的代码，自定义还是比较简单。</p>
<pre><code class="language-python">
import os
import numpy as np
from torch.utils.data import DataLoader,Dataset
from torchvision.datasets.folder import default_loader


class CustomDataset(Dataset):
    def __init__(self,
                 img_path,
                 txt_path,
                 loader = default_loader,
                 img_transform=None,
                 ):
        with open(txt_path, 'r') as f:
            lines = f.readlines()
            self.imgs = [
                os.path.join(img_path, i.split(',')[0].partition('\\')[2]) for i in lines
            ]
            self.label_list = [i.split('\\')[1] for i in lines]
            self.feature_list = np.array([list(map(float,[i.split(',')[1],i.split(',')[2],
                                            i.split(',')[3],i.split(',')[4],
                                            i.split(',')[5],i.split(',')[6]]))
                                 for i in lines])
        self.feature_list[:,2:] = self.feature_list[:,2:] / 28
        self.img_transform = img_transform
        self.loader = loader
        self.labels = list(set(self.label_list))
        self.labels.sort()
        self.class_to_idx = dict(zip(self.labels ,range(len(self.labels))))
        self.label_list = [self.class_to_idx[c] for c in self.label_list]

    def __getitem__(self, index):
        img_path = self.imgs[index]
        extra_feature = self.feature_list[index]
        label = self.label_list[index]
        img = self.loader(img_path)
        if self.img_transform is not None:
            img = self.img_transform(img)
        return img, extra_feature, label

    def __len__(self):
        return len(self.label_list)

</code></pre>
<h1 id="libtorch下">libtorch下</h1>
<p>libtorch下模型的forward方法的输入是一个向量，如果模型的forward方法每个参数对应一个输入的话，在对应位置输入就没问题，但是如果python模型使用了tuple来传inputs，那可能遇到下面几种参数不匹配的错误，在libtorch中调用模型的forward时无法将输入的std::vector&lt;torch::IValue&gt;转换为(Tensor, Tensor)。</p>
<pre><code>Expected value of type (Tensor, Tensor) for argument 'argument_1' in position 0, but instead got value of type Tensor. Declaration: forward((Tensor, Tensor) argument_1) -&gt; Tensor
</code></pre>
<p>或者</p>
<pre><code>Expected at most 1 argument(s) for operator 'forward', but received 2 argument(s). Declaration: forward((Tensor, Tensor) argument_1) -&gt; Tensor 
</code></pre>
<p>另外还有在torch.jit.trace阶段可能碰到的错误，可以多包裹一层tuple解决，例如((sample_input_1, sample_input_2),)。</p>
<pre><code>TypeError: forward() takes 2 positional arguments but 3 were given</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Pooling]]></title>
        <id>https://txSangyj.github.io/post/universal-pooling/</id>
        <link href="https://txSangyj.github.io/post/universal-pooling/">
        </link>
        <updated>2019-07-30T01:57:38.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
逛Paperweekly看到的一篇论文，只用过池化还没看过相关论文。看完也没有太惊艳的感觉，可能池化操作还是不被看好吧。InceptionV4中的Reduction Block也是Maxpooling和卷积层同时使用，GAN为了不丢失信息也是卷积层替代，除了Global average pooling (GAP)这种替代全连接层的操作。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E6%B1%A0%E5%8C%96">池化</a>
<ul>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%B1%A0%E5%8C%96">常用池化</a></li>
<li><a href="#%E6%B1%A0%E5%8C%96%E4%BD%9C%E7%94%A8">池化作用</a></li>
</ul>
</li>
<li><a href="#universal-pooling">Universal Pooling</a></li>
<li><a href="#%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98">存在的问题</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<br>
逛Paperweekly看到的一篇论文，只用过池化还没看过相关论文。看完也没有太惊艳的感觉，可能池化操作还是不被看好吧。InceptionV4中的Reduction Block也是Maxpooling和卷积层同时使用，GAN为了不丢失信息也是卷积层替代，除了Global average pooling (GAP)这种替代全连接层的操作。</p>
<!-- more -->
<h1 id="池化">池化</h1>
<p>简单的卷积神经网络一般包括卷积层、池化层以及全连接层。池化在其中是一种降采样的过程，主要分为平均池化和最大池化。</p>
<h2 id="常用池化">常用池化</h2>
<p>常见的就是Maxpooling和Meanpooling，容易理解，直接上图。<br>
Maxpooling:<br>
<img src="https://i.loli.net/2019/07/31/5d40f8c5cf32a37790.png" alt="Maxpooling" loading="lazy"><br>
Meanpooling/Averagepooling:<br>
<img src="https://i.loli.net/2019/07/31/5d40f91906e0b64625.png" alt="Meanpooling" loading="lazy"></p>
<h2 id="池化作用">池化作用</h2>
<ul>
<li>降采样，降低参数量和计算量。</li>
<li>增大后续层单元的感受野。</li>
<li>降低微弱噪声和畸变的影响</li>
</ul>
<h1 id="universal-pooling">Universal Pooling</h1>
<p>提出的通用池化层可以生成任意类型的池化，核心类似注意力机制。<br>
<strong>核心</strong>：Universal pooling can be considered as a channel-wise local spatial attention module.</p>
<blockquote>
<p>The basic idea of universal pooling is to interpret pooling as attention and extend it to the general channelwise local spatial attention. That is, the  universal pooling selects pooling weights for each channel and they are <strong>trained</strong> together with other feature extraction parts.</p>
</blockquote>
<p>定义的池化操作的限制：</p>
<ol>
<li>channel-wise:每个channel分开来训练权重参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>。</li>
<li>仅池化给定块</li>
<li>池化不能改变特征的尺度（块内权重之和为1）<br>
<img src="https://i.loli.net/2019/07/30/5d3fda16bc1f829435.jpg" alt="结构" loading="lazy"><br>
结构上Block1.1是一个内部神经网络，还是可以使用卷积层、全连接层、批归一化层等进行构建，神经网络的输入是单个特征图，输出为临时特征图，尺寸和输入相同，经过block1.2进行块内（形状由步长决定）的softmax转化为各分块的内部权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>，然后Pooling map和Feature map作点积、块内求和等操作得到池化后的输出。</li>
</ol>
<h1 id="存在的问题">存在的问题</h1>
<ol>
<li>池化操作使用较少，特征图的尺度变换基本上由卷积层的stride和padding控制。</li>
<li>增加了参数量和计算量，附加了一个内部的神经网络，带来了额外的结构设计问题。</li>
<li>学习MaxPooling这种高度非线性化的函数可能存在困难。</li>
</ol>
<hr>
<h1 id="参考">参考</h1>
<p><a href="https://arxiv.org/abs/1907.11440">论文地址</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[C++ lambda]]></title>
        <id>https://txSangyj.github.io/post/c-lambda/</id>
        <link href="https://txSangyj.github.io/post/c-lambda/">
        </link>
        <updated>2019-07-18T06:49:17.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
C++中lambda的介绍和简单使用。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E6%9E%84%E6%88%90">构成</a>
<ul>
<li><a href="#captures">captures</a></li>
<li><a href="#params">params</a></li>
<li><a href="#ret">ret</a></li>
<li><a href="#body">body</a></li>
</ul>
</li>
<li><a href="#%E4%BE%8B%E5%AD%90">例子</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<br>
C++中lambda的介绍和简单使用。</p>
<!-- more -->
<h1 id="构成">构成</h1>
<p>C++11下lambda表达式完全体的构成：<br>
[ captures ]  ( params ) specifiers exception attr -&gt; ret { body }</p>
<p>其中params和ret可以省略，省略返回值类型相当于返回auto类型，省略（params）相当于lambda函数不接受参数输入，对应以下几种变体：</p>
<ul>
<li>[ captures ] ( params ) -&gt; ret { body }</li>
<li>[ captures ] ( params ) { body }</li>
<li>[ captures ] { body }</li>
</ul>
<h2 id="captures">captures</h2>
<p>支持值捕获、引用捕获和隐式捕获。<br>
<strong>值捕获和引用捕获</strong><br>
在变量前使用前缀=来表示值捕获，&amp;表示引用捕获。变量在captures列表只能出现一次，当变量被一种方式隐式捕获时，显示捕获只能将其声明为另一种捕获方式。<br>
【1】中的例子：</p>
<pre><code class="language-c++">// 默认引用捕获
struct S2 { void f(int i); };
void S2::f(int i)
{
    [&amp;]{};          // OK: by-reference capture default
    [&amp;, i]{};       // OK: by-reference capture, except i is captured by copy
    [&amp;, &amp;i] {};     // Error: by-reference capture when by-reference is the default
    [&amp;, this] {};   // OK, equivalent to [&amp;]
    [&amp;, this, i]{}; // OK, equivalent to [&amp;, i]
}
</code></pre>
<pre><code class="language-c++">// 默认值捕获
struct S2 { void f(int i); };
void S2::f(int i)
{
    [=]{};          // OK: by-copy capture default
    [=, &amp;i]{};      // OK: by-copy capture, except i is captured by reference
    [=, *this]{};   // until C++17: Error: invalid syntax
                    // since c++17: OK: captures the enclosing S2 by copy
    [=, this] {};   // until C++20: Error: this when = is the default
                    // since C++20: OK, same as [=]
}
</code></pre>
<p><strong>隐式捕获</strong><br>
上面的例子中可以看到，当捕获列表的首项为=及&amp;时会按相应的捕获方式捕获所用的变量和this下的成员变量。另外lambda表达式可以不捕获而使用或读取一些变量，比如全局变量、静态局部变量等，具体类型可以查看参考[1]。</p>
<pre><code class="language-c++">
#include &lt;iostream&gt;
using namespace std;

int i = 10;
int main()
{
    static int j = 1;
    auto f = []() {cout &lt;&lt;&quot;i:&quot;&lt;&lt; i &lt;&lt;&quot;\tj:&quot;&lt;&lt; j; };
    f();
}
// out:
// i:10	j:1
</code></pre>
<h2 id="params">params</h2>
<p>和函数参数差不多，在C++14后不支持指定参数默认值且支持用auto声明参数。<br>
<strong>参数名与变量名相同会隐藏所捕获的变量。</strong></p>
<h2 id="ret">ret</h2>
<p>返回值类型，可省略。省略后要注意不同分支下返回值类型要保持一致。</p>
<h2 id="body">body</h2>
<p>函数体。当变量为值捕获时，要在lambda表达式的parameter list（不可省略） 和 return type（省略或&quot;-&gt;ret&quot;形式）之间指定说明符为mutable才能修改变量的值，但仍是值引用。</p>
<pre><code class="language-c++">
int i = 0;
auto f = [i]() mutable {return ++i; };
cout &lt;&lt; &quot;i:&quot; &lt;&lt; i &lt;&lt; &quot;\tf():&quot; &lt;&lt; f();

// out:
// i:0     f():1
</code></pre>
<h1 id="例子">例子</h1>
<p>在一些需要输入函数的时候就可以用lambda生成匿名函数传入，比如std中的sort、for_each等函数。</p>
<pre><code class="language-c++">int main()
{
    using namespace std;
    vector&lt;int&gt; nums = { 1,2,3,4,5,6,7 };

    // 使用lambda和for_each定义一个函数用于输出vector
    auto print_vec = [](const vector&lt;int&gt; vec) {
        for_each(vec.begin(), vec.end(), [](int i) {cout &lt;&lt; i &lt;&lt; '\t'; });
    };
    print_vec(nums);
    cout &lt;&lt; endl;

    // 引用方式修改值
    for_each(nums.begin(), nums.end(), [](int&amp; i) { i -= 4; });
    print_vec(nums);
    cout &lt;&lt; endl;

    // sort自定义排序
    sort(nums.begin(), nums.end(), [](int a, int b) -&gt; bool { return abs(a) &lt; abs(b); }); 
    print_vec(nums);
    cout &lt;&lt; endl;
}
// out:
// 1       2       3       4       5       6       7
// -3      -2      -1      0       1       2       3
// 0       -1      1       -2      2       -3      3
</code></pre>
<hr>
<h1 id="参考">参考</h1>
<p>[1]<a href="https://zh.cppreference.com/w/cpp/language/lambda">cppreference-lambda</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hexo 转到 Gridea]]></title>
        <id>https://txSangyj.github.io/post/hexo-zhuan-dao-gridea/</id>
        <link href="https://txSangyj.github.io/post/hexo-zhuan-dao-gridea/">
        </link>
        <updated>2019-07-17T06:36:21.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
逛论坛时候发现个新玩具<a href="https://gridea.dev/">Gridea</a>，然后就把原来Hexo下的东西迁到了Gridea下。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E9%85%8D%E7%BD%AE">配置</a></li>
<li><a href="#%E6%96%87%E7%AB%A0">文章</a></li>
<li><a href="#%E5%9B%BE%E7%89%87">图片</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98">其他问题</a></li>
</ul>
<br>
逛论坛时候发现个新玩具<a href="https://gridea.dev/">Gridea</a>，然后就把原来Hexo下的东西迁到了Gridea下。</p>
<!-- more -->
<h1 id="配置">配置</h1>
<p>上教程<a href="https://gridea.dev/docs/">Gridea doc</a></p>
<h1 id="文章">文章</h1>
<p>原Hexo的source/_post下的.md文件复制到Gridea源文件夹下的posts下。注意posts下<strong>不能</strong>有文件夹。<s>可能是Bug吧。</s></p>
<h1 id="图片">图片</h1>
<p>又不能有asset_folder，Gridea插入的图片使用的又是绝对路径，所以把之前的图片转成了图床链接。用的是<a href="https://github.com/kookob/smpic">Simpic</a>上传。</p>
<h1 id="其他问题">其他问题</h1>
<ul>
<li>原来的主题不会适配。</li>
<li>文章的分类没有自动识别。</li>
<li>Gridea下写东西<strong>千万不要随便点返回</strong>。</li>
<li>建议文章都添加&lt;!-- more --&gt;不然有的主页显示会出现错位。</li>
<li>字数统计、点击统计好像暂不支持。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[python 多线程/多进程]]></title>
        <id>https://txSangyj.github.io/post/multithreaded-multiprocessing/</id>
        <link href="https://txSangyj.github.io/post/multithreaded-multiprocessing/">
        </link>
        <updated>2018-12-28T11:55:14.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
进程和线程区别也是常问的问题，这也记一下。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B">进程和线程</a>
<ul>
<li><a href="#%E8%BF%9B%E7%A8%8B">进程</a></li>
<li><a href="#%E7%BA%BF%E7%A8%8B">线程</a></li>
<li><a href="#%E5%8C%BA%E5%88%AB">区别</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B">多线程</a>
<ul>
<li><a href="#gil">GIL</a></li>
<li><a href="#threading">threading</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B">多进程</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<br>
进程和线程区别也是常问的问题，这也记一下。</p>
<!-- more -->
<h1 id="进程和线程">进程和线程</h1>
<h2 id="进程">进程</h2>
<p>可执行的程序加载到内存中，系统为它分配资源后运行中的程序称之为进程。进程是程序关于某数据集合的一次运行活动，是操作系统进行资源分配（地址空间、内存、数据栈）和调度的基本单位。</p>
<h2 id="线程">线程</h2>
<p>线程被包含在进程中，是进程中的实际运作单位，是操作系统进行调度的最小单位。线程是属于进程的，同一进程下的线程共享相同的运行环境。</p>
<h2 id="区别">区别</h2>
<ol>
<li>
<p>进程是资源分配的单位；线程是CPU调度的单位。</p>
</li>
<li>
<p>进程间切换代价大；线程间切换代价小。</p>
</li>
<li>
<p>进程拥有资源多；线程拥有资源少。</p>
</li>
<li>
<p>进程间不能直接共享信息；线程间可以通过共享数据通信。</p>
</li>
<li>
<p>一个进程死掉不影响其他进程；进程下的线程死掉会导致进程死掉。</p>
<blockquote>
<p>（所以写多进程的时候总是同样的错报四次hhh）</p>
</blockquote>
</li>
</ol>
<h1 id="多线程">多线程</h1>
<h2 id="gil">GIL</h2>
<p>Python代码执行由Python虚拟机控制，而对Python虚拟机的访问由全局解释器锁GIL控制，它保证即使在多核心处理器上同一时刻也只有一个线程在运行。多线程环境下虚拟机的运行方式是（获得锁-切换到一个线程运行-间隔检查/sleep-睡眠-解锁）的循环。</p>
<p>线程在处理I/O调用的时候会释放GIL，而CPU密集型的线程会在自己的时间片内一直占用处理器。所以CPU密集型的任务不推荐使用多线程。</p>
<h2 id="threading">threading</h2>
<p>threading的Thread类提供两种方式创建线程：1、传递一个可调用对象；2、派生一个子类，重写run方法。</p>
<pre><code class="language-python">import threading
from time import sleep,ctime

def loop(x):
    t = ctime()
    sleep(x)
    print(&quot;{0} start at {1},stop at {2}&quot;.format(
        threading.current_thread().getName(),t,ctime()))
    return x

class MyThread(threading.Thread):
    
    def __init__(self, target=None ,args=()):
        self.result = None
        return super().__init__(target=target,args=args)
    
    #使用子类继承的好处就是更加灵活
    def getResult(self):
        return self.result
    
    def run(self):
        self.result = self._target(*self._args)

if __name__ == '__main__':
    
    #使用Thread类
    for i in range(4):
        mythread = threading.Thread(target = loop,args = (i,))
        mythread.start()
    sleep(4)
    #使用MyThread类
    thread_list = []
    for i in range(4):
        my_thread = MyThread(target=loop,args=(i,))
        my_thread.start()
        thread_list.append(my_thread)
    
    for t in thread_list:
        t.join()
    for t in thread_list:
        print(t.getResult())
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://i.loli.net/2019/07/16/5d2de1b3af4d466932.png" alt="运行截图" loading="lazy"></figure>
<ul>
<li>print函数不是线程安全的，可以用sys.stdout.write方法替换。</li>
<li>这段没有线程通信，参考里有讲的很详细的文章。</li>
</ul>
<h1 id="多进程">多进程</h1>
<p>Python多进程使用的是multiprocessing包，在接口定义上与threading大致相同。</p>
<pre><code>import multiprocessing as mp   
import sys
from time import sleep,ctime

def loop(x):
    t = ctime()
    sleep(x)
    sys.stdout.write(&quot;{0} start at {1},stop at {2}\n&quot;.format(
        mp.process.current_process(),t,ctime()))

    return x

if __name__ == '__main__':
    
    #类似于threading的用法
    for i in range(4):
        my_process = mp.Process(target = loop,args = (i,))
        my_process.start()
    sleep(4)

    #使用进程池，可以获取函数的返回值
    pool = mp.Pool()
    res = pool.map(loop,range(4)) #map接收可迭代对象
    print(res)
    
    #阻塞执行的apply
    res_list = [pool.apply(loop,(i,)) for i in range(4)]
    print(res_list)
    
    #异步非阻塞的apply_async
    res_list = [pool.apply_async(loop,(i,)).get() for i in range(4)]
    print(res_list)
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://i.loli.net/2019/07/16/5d2de1d4531ef71329.png" alt="运行截图" loading="lazy"></figure>
<p>多进程有一个坑点，进程间pickle进行数据序列化传递数据，然而有些数据不能被序列化，图片来自[5]。</p>
<figure data-type="image" tabindex="3"><img src="https://i.loli.net/2019/07/16/5d2de1bc235d083052.png" alt="能被序列化的对象" loading="lazy"></figure>
<p>再引一张进程池工作方式的图，感觉很有用，来自[4]。</p>
<figure data-type="image" tabindex="4"><img src="https://i.loli.net/2019/07/16/5d2de1e40b90751549.png" alt="进程池工作方式" loading="lazy"></figure>
<h1 id="参考">参考</h1>
<p>[1] 了解GIL看这里http://cenalulu.github.io/python/gil-in-python/</p>
<p>[2] 多线程这篇内容更全面http://codingpy.com/article/python-201-a-tutorial-on-threads/</p>
<p>[3] 多进程pickle问题解决看这里https://strcpy.me/index.php/archives/318/</p>
<p>[4] 进程池源码解析http://www.codexiu.cn/python/blog/939/</p>
<p>[5] https://docs.python.org/2/library/pickle.html#what-can-be-pickled-and-unpickled</p>
<p>PPS终于重新开始记笔记了。</p>
]]></content>
    </entry>
</feed>