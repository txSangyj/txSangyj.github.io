<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://txSangyj.github.io/</id>
    <title>ji</title>
    <updated>2023-04-12T15:55:52.103Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://txSangyj.github.io/"/>
    <link rel="self" href="https://txSangyj.github.io/atom.xml"/>
    <subtitle>学习用</subtitle>
    <logo>https://txSangyj.github.io/images/avatar.png</logo>
    <icon>https://txSangyj.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, ji</rights>
    <entry>
        <title type="html"><![CDATA[利用AI学习Rust]]></title>
        <id>https://txSangyj.github.io/post/li-yong-ai-xue-xi-rust/</id>
        <link href="https://txSangyj.github.io/post/li-yong-ai-xue-xi-rust/">
        </link>
        <updated>2023-04-12T15:17:52.000Z</updated>
        <summary type="html"><![CDATA[<p>前一段时间Python的类型注解写的实在太难受了，想起了之前看过的rust，重新捡起来看看，然后学到了AI时代的学习方法。</p>
]]></summary>
        <content type="html"><![CDATA[<p>前一段时间Python的类型注解写的实在太难受了，想起了之前看过的rust，重新捡起来看看，然后学到了AI时代的学习方法。</p>
<!-- more -->
<h2 id="简单几步">简单几步</h2>
<ol>
<li>首选当然是读基础的文档，<a href="https://kaisery.github.io/trpl-zh-cn/">Rust程序设计语言</a></li>
<li>看完不知道做啥，刷点题吧，找一位大佬的<a href="https://github.com/iosmanthus/leetcode-rust">刷题仓库</a>。</li>
<li>vscode安装拓展<a href="https://github.com/mintlify/writer">Mintlify Writer</a>，一个AI生成自动生成文档的插件。</li>
<li>看不懂就选中再按<code>Ctrl+.</code>。<s>哪里不会点哪里！</s></li>
</ol>
<h2 id="看看效果">看看效果</h2>
<p>选中<code>ListNode</code>、<code>derive</code>、<code>inline</code>、<code>Box</code>、<code>unwrap_or</code>都可以生成对应的说明。放到一起是有点混乱，Rust的docstring是markdown格式的，可以很方便摘抄出来。<br>
<code>common/src/lib.rs</code></p>
<pre><code class="language-rust">// Definition for singly-linked list.
pub mod structs {
    /**
    Defines a struct named ListNode with two fields, val of type i32 and next of type
    Option&lt;Box&lt;ListNode&gt;&gt;.

    Properties:

    * `val`: The `val` property of the `ListNode` struct represents the value stored in the node. In
    this case, it is an integer value of type `i32`.
    * `next`: `next` is a property of the `ListNode` struct that represents a pointer to the next node
    in a linked list. It is an `Option` type that can either be `None` if there is no next node, or
    `Some` containing a `Box` that points to the next
    
    `#[derive(PartialEq,Eq,Debug)]` is a Rust attribute that automatically generates implementations
    of the `PartialEq`, `Eq`, and `Debug` traits for the `ListNode` struct. This allows instances of
    the `ListNode` struct to be compared for equality using the `==` operator, and printed using the
    `println!` macro with the `{:?}` format specifier.
    */
    #[derive(PartialEq,Eq,Debug)]
    pub struct ListNode{
        pub val: i32,
        pub next: Option&lt;Box&lt;ListNode&gt;&gt;,
    }
    impl ListNode{
        /**
        `#[inline]` is a Rust attribute that suggests the compiler to inline the function at the
        call site. This means that instead of calling the function, the code of the function is
        inserted directly into the calling code. This can improve performance by reducing the
        overhead of function calls. 
        */
        #[inline]
        pub fn new(val: i32) -&gt; Self {
            ListNode { next: None, val}
        }
    }
}


#[cfg(test)]
mod tests {
    use super::structs::*;
    #[test]
    /**
    `Box` is a smart pointer in Rust that provides **heap** allocation of
    values. In this code, `Box` is used to create a new instance of
    `ListNode` on the heap and store a pointer to it in the `next` field
    of another `ListNode`. This allows for the creation of a linked list
    data structure where each node is allocated on the heap and connected
    to the next node through a pointer.
    */
    /**
    `unwrap_or` is a method provided by the `Option` type in Rust. It
    takes a default value as an argument and returns either the value
    contained in the `Option` if it is `Some`, or the default value if
    the `Option` is `None`. In the given code,
    `result.take().unwrap_or(Box::new(ListNode::new(0))).val` is used
    to get the value of the `ListNode` contained in `result`, or a new
    `ListNode` with a value of 0 if `result` is `None`. The `val`
    property of the resulting `ListNode` is then returned.
    */
    fn it_works() {
        let mut result = Some(
        Box::new(ListNode {
            val: 9,
            next: Some(Box::new(ListNode {
                val: 8,
                next: Some(Box::new(ListNode {
                    val: 3,
                    next: Some(Box::new(ListNode::new(3))),
                })),
            })),
        }));

        assert_eq!(result.take().unwrap_or(Box::new(ListNode::new(0))).val, 9);
    }
}
</code></pre>
<p>插件有个问题是选中的内容是行内的话，生成注释会破坏原来的格式，需要手动调整下。<br>
当然自动生成的内容基本只是解释现有代码， 不涉及更深层次的内容，还需要结合其他方式学习。<br>
就这样~</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ 2022书单]]></title>
        <id>https://txSangyj.github.io/post/2022-shu-dan/</id>
        <link href="https://txSangyj.github.io/post/2022-shu-dan/">
        </link>
        <updated>2023-01-03T11:16:01.000Z</updated>
        <summary type="html"><![CDATA[<p>今年的总结早多了。</p>
]]></summary>
        <content type="html"><![CDATA[<p>今年的总结早多了。</p>
<!-- more -->
<h1 id="已">已</h1>
<h2 id="路边野餐">《路边野餐》</h2>
<pre><code>当时野餐露营挺火。
</code></pre>
<p>苏联时代的科幻小说，人名过于拗口了。确实几个人像工蚁一样。附录还有作者对主编修改意见的吐槽，笑死。</p>
<h2 id="神们自己">《神们自己》</h2>
<p>赶上了隔离的尾巴，在酒店隔离时候读的。</p>
<pre><code>拉蒙特发现了电子通道存在的问题，想关闭电子通道，但关闭意味倒退。
杜阿文明关系文明存亡，更不可能关闭。
狄尼特不光指出了问题，也提出了解决问题的方法。
</code></pre>
<p>拉蒙特和政治家的对话。</p>
<blockquote>
<p>“年轻人，我的权力从名义上来说很大，但是我只能在符合公众愿望的情况下才拥有这么大的权力。”</p>
</blockquote>
<h2 id="教父-parti">《教父 PART:Ⅰ》</h2>
<p>文字细节多了点，但电影确实太还原了。<br>
图书馆只有Ⅰ和Ⅲ，读完Ⅰ就没有然后了。</p>
<h2 id="兄弟">《兄弟》</h2>
<p>看了余华和罗翔的对谈去看的。<br>
上半部为宋凡平难受流了不知道多少泪，下半部看刘镇众人变化，气愤于宋钢的随波逐流。<br>
偷别人的书评：</p>
<blockquote>
<p>上半部真实的荒诞<br>
下半部荒诞的真实</p>
</blockquote>
<h2 id="皮囊">《皮囊》</h2>
<p>写作者身边的人，散文印象不深。<br>
父亲的瘫痪 倔强 偏执 精神和肉体的错位<br>
母亲的坚毅 执念 走后的思念<br>
神婆 小小</p>
<h2 id="两篇网文">两篇网文</h2>
<h3 id="深空之流浪舰队">《深空之流浪舰队》</h3>
<p>实在是太长了，跳过不少段落，只有一句话：</p>
<blockquote>
<p>愿不断进步。</p>
</blockquote>
<h3 id="我们生活在南京">《我们生活在南京》</h3>
<p>铺垫转折不断，篇幅不长好评。梗太多偶尔出戏但读起来相对轻松，<s>通信课外读物</s>。</p>
<h1 id="正">正</h1>
<h2 id="树犹如此">《树犹如此》</h2>
<h2 id="少年pi的奇幻漂流">《少年Pi的奇幻漂流》</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一个简单的脚本]]></title>
        <id>https://txSangyj.github.io/post/a/</id>
        <link href="https://txSangyj.github.io/post/a/">
        </link>
        <updated>2021-04-16T07:37:15.000Z</updated>
        <summary type="html"><![CDATA[<p>迫于被远程桌面输命令疯狂重复按键折磨吐了写的，不过这样自动补全没了。<s>好在不需要再用了</s></p>
]]></summary>
        <content type="html"><![CDATA[<p>迫于被远程桌面输命令疯狂重复按键折磨吐了写的，不过这样自动补全没了。<s>好在不需要再用了</s></p>
<!-- more -->
<pre><code class="language-python"># test.py
import win32gui,win32api,win32con
import time

def send_to_cmd():
    wndtitle = u&quot;&quot; # 窗口名称，可以spy++获取
    wndclass = None 
    wnd = win32gui.FindWindow(wndclass, wndtitle)
    win32api.keybd_event(13, 0, 0, 0)
    win32gui.SetForegroundWindow(wnd)
    win32api.SetCursorPos((1000, 200)) # 鼠标位置1
    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTDOWN, 100, 100, 0, 0)
    time.sleep(0.2)
    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTUP, 100, 100, 0, 0)
    
    wndtitle = u&quot;C:\WINDOWS\system32\cmd.exe - python  test.py&quot;
    wndclass = None 
    wnd = win32gui.FindWindow(wndclass, wndtitle)
    win32api.keybd_event(13, 0, 0, 0) #
    win32gui.SetForegroundWindow(wnd)
    win32api.SetCursorPos((100, 100)) # 鼠标位置2
    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN, 100, 100, 0, 0)
    time.sleep(0.2)
    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP, 100, 100, 0, 0)
    
    
def inputtxt(string):
    import win32clipboard as w
    w.OpenClipboard()
    w.SetClipboardData(win32con.CF_UNICODETEXT,string)
    w.CloseClipboard()

if __name__=='__main__':
    while True:
        s = input()
        if s!='quitcmd':
            inputtxt(s+'\n')
            send_to_cmd()
        else:
            break
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[对话分类竞赛总结]]></title>
        <id>https://txSangyj.github.io/post/dui-hua-fen-lei-jing-sai-zong-jie/</id>
        <link href="https://txSangyj.github.io/post/dui-hua-fen-lei-jing-sai-zong-jie/">
        </link>
        <updated>2021-03-05T01:33:52.000Z</updated>
        <summary type="html"><![CDATA[<p>一篇旧文。</p>
]]></summary>
        <content type="html"><![CDATA[<p>一篇旧文。</p>
<!-- more -->
<h1 id="总结">总结</h1>
<h2 id="初赛">初赛</h2>
<p>第一次接触NLP问题，初赛时间较为充裕，而Bert处理短文本性能足够。应尽量尝试多种可行方式解决问题，而不是只拿着Bert跑无意义的结果。</p>
<h2 id="复赛决赛">复赛&amp;决赛</h2>
<ul>
<li>复赛阶段数据量大，试错成本更高，完全应该进行大的改动再重新迭代。</li>
<li>数据处理和基础模型没有大问题，但是完全忘记了在提交结果阶段使用<strong>集成学习</strong>方法。</li>
<li>没有对长文本进行进一步处理，取首尾截断不如训练两个模型<strong>分别处理首尾</strong>之后再进行投票。</li>
<li>程序总是有Bug，一个on_epoch_end的问题就是九个小时左右时间的浪费。</li>
<li>batch_size/model 过大，训练时无法测试</li>
<li>虽然做了客服标记的统一化，但是标记本身是否正确未可知。</li>
<li>复赛阶段工作量预估不足。</li>
</ul>
<h2 id="其他">其他</h2>
<ul>
<li>文本是机器翻译得来的， 可以进一步处理。（将方言转化得到的无意义的同音词替换为行业高频用词是一个好方向）</li>
<li>或者将对话双方文本分别训练也是一个方向。</li>
<li>长文本甚至可以做一个摘要生成的模型来处理成短文本。</li>
<li>BiLSTM+Attention机制没有做完，或者在BiLSTM各时间步上隐层上取Max做输出，在其他数据集上效果均优于直接使用BiLSTM。</li>
<li>梯度累加变相增大Batch_size的实验没有做完，新建Module类可以解决loss和原计算图无法关联的问题。（较小的模型比如共享参数的TextCNN可以做到256的batch_size，16倍于bert方法）</li>
<li>Bert本身没有问题，但是训练用的语料不同效果不同，也可能影响最后效果。</li>
</ul>
<h2 id="后续应用">后续应用</h2>
<ul>
<li>
<p>落地方面思考不足，自然语言处理工作在客服对话上可用的有：</p>
<ol>
<li>投诉分类（将和故障高度相关的投诉分类抽取，设置告警（一个问题是误告警怎么闭环处理，没有进行一个较好的回答）</li>
<li>多分类，客户可能存在多种需求，单个标签描述不便（程序上改为输出层sigmoid激活，使用二元分类损失，按每类概率&gt;0.5取对应标签。</li>
<li>不局限于分类，语义理解聚类客户诉求、根据对话方式进行用户分类匹配客服、为什么会出现长对话（话术提炼、优化效率）。</li>
</ol>
</li>
<li>
<p>TensorRT推理速度有极大提高（成百倍500ms-&gt;2ms），可以尝试。</p>
</li>
</ul>
<hr>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2021书单]]></title>
        <id>https://txSangyj.github.io/post/2021-shu-dan/</id>
        <link href="https://txSangyj.github.io/post/2021-shu-dan/">
        </link>
        <updated>2021-03-03T14:53:31.000Z</updated>
        <summary type="html"><![CDATA[<p>都三月了，书看了几本？<br>
2022.3.7 Update 回顾一下。</p>
]]></summary>
        <content type="html"><![CDATA[<p>都三月了，书看了几本？<br>
2022.3.7 Update 回顾一下。</p>
<!-- more -->
<h2 id="已">已：</h2>
<h3 id="娱乐至死">《娱乐至死》</h3>
<p>像看一篇论文。不看抖音，看完微博都想卸了。应该控制一下获取无用信息的频率了。<br>
回想起来私货很多。</p>
<h3 id="小狗钱钱12">《小狗钱钱》1&amp;2</h3>
<pre><code>    1. 小目标，开家小书店要多少钱？
    2. 养鹅🐧
    3. “这真的有必要吗”
    4. 周四记账
    5. 第二册更像“儿童读物”
</code></pre>
<h2 id="在">在：</h2>
<h3 id="伟大作品的隐秘结构">《伟大作品的隐秘结构》</h3>
<p>无结论的两难结构：举例有《伽利略传》、《老人与海》等等。<s>《我们与恶的距离》算不算...</s><br>
<s>半透明的双层结构：还没看到。。。</s></p>
<blockquote>
<p>艺术创造靠一种神奇的虚设触及了人们的两重共性：一是所刻画的对象在人们中的共性；二是欣赏者内心的某种共性。</p>
</blockquote>
<pre><code>艺术眼光不是政治眼光里提到契诃夫的《万卡》，随手一搜发现《山海情》里念的课文就是这个。
</code></pre>
<blockquote>
<p>后面发现是教材，看了个序？</p>
</blockquote>
<h3 id="沙丘"><s>《沙丘》</s></h3>
<h2 id="~~读译文真是痛苦不知道能不能坚持读完-~~果然没看完-202237-update">~~读译文真是痛苦，不知道能不能坚持读完。~~果然没看完。<br>
2022.3.7 Update</h2>
<h3 id="丧钟为谁而鸣">《丧钟为谁而鸣》</h3>
<p>2077里面带出来的。不过也没读完，进度50%吧。</p>
<h3 id="风声">《风声》</h3>
<p>回顾完电影去看，发现内容居然差别有点大。</p>
<h3 id="文化苦旅">《文化苦旅》</h3>
<p>10+年后又看王道士。</p>
<h3 id="白鹿原">《白鹿原》</h3>
<p>看完气的吃不下饭。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[aarch64下编译libtorch]]></title>
        <id>https://txSangyj.github.io/post/armv8-bian-yi-libtorch/</id>
        <link href="https://txSangyj.github.io/post/armv8-bian-yi-libtorch/">
        </link>
        <updated>2019-08-15T02:32:27.000Z</updated>
        <summary type="html"><![CDATA[<p>把写的libtorch调用模型的程序移植到了下Ubuntu下发现没有问题，libtorch提供了预编译好的库。放到基于Arm的国产主机上就提示libtorch.so不是动态库文件，需要从源代码编译libtorch。不过没啥难点，踩坑文。</p>
]]></summary>
        <content type="html"><![CDATA[<p>把写的libtorch调用模型的程序移植到了下Ubuntu下发现没有问题，libtorch提供了预编译好的库。放到基于Arm的国产主机上就提示libtorch.so不是动态库文件，需要从源代码编译libtorch。不过没啥难点，踩坑文。</p>
<!-- more -->
<h1 id="获取源码">获取源码</h1>
<pre><code class="language-bash">
git clone https://github.com/pytorch/pytorch --recursive &amp;&amp; cd pytorch
git checkout v1.2.0 # 真的勇士敢于在master分支下编译
#下载编译需要的子模块
git submodule sync
git submodule update --init --recursive
</code></pre>
<h1 id="python环境">python环境</h1>
<p>pytorch官方给出的环境是用conda装的，然而悲剧的是官方并没有给出aarch64的安装文件，所以还是使用pip来安装。还有个问题是很多python库并没有发布对应架构下的包，好在编译libtorch需要的python环境较为简单。编译完之后导出python环境发现，编译libtorch必须的包应该只有pyyaml（不太确定）。当然cmake和gcc等工具还是必须的。<br>
安装pyyaml的命令：</p>
<pre><code>pip install pyyaml
</code></pre>
<blockquote>
<p>不使用tools下的脚本而直接用cmake来编译的话可能python环境都不需要了...</p>
</blockquote>
<h1 id="编译libtorch">编译libtorch</h1>
<p>在编译之前可以使用export或者cmake-gui关闭一些不必要的编译选项来加快编译速度，类似：</p>
<pre><code>export USE_CUDA=False
export BUILD_TEST=False
</code></pre>
<p>官方提供了一键式编译工具，在pytorch/tools/build_libtorch.py。只需要运行：</p>
<pre><code class="language-bash">
#pytorch$
mkdir build &amp;&amp; cd build
python ../tools/build_libtorch.py
</code></pre>
<p>就会在build下生成对应的文件，我们需要的动态库文件libtorch.so在build/lib/下。</p>
<h1 id="整理libtorch">整理libtorch</h1>
<ol>
<li>复制pytorch下torch、caffe2、c10以及aten\source下的ATen、TH等文件夹到libtorch/include下，得到libtorch的C++接口。</li>
<li>复制build/lib下编译好的库文件到libtorch/lib下。</li>
<li>复制pytorch/torch/share/cmake文件夹到libtorch/share下。<br>
得到的libtorch主要结构：<br>
libtorch.<br>
├─include<br>
│  ├─ATen<br>
│  ├─c10<br>
│  ├─caffe2<br>
│  ├─TH<br>
│  ├─THCUNN<br>
│  ├─THNN<br>
│  └─torch<br>
├─lib<br>
└─share<br>
└─cmake<br>
├─ATen<br>
├─Caffe2<br>
├─Gloo<br>
└─Torch</li>
</ol>
<hr>
<p>如果你需要pytorch编译可以参考一下<a href="https://nmilosev.svbtle.com/compling-arm-stuff-without-an-arm-board-build-pytorch-for-the-raspberry-pi">这篇</a><br>
ps：还是对CMakeLists不太熟啊。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[torch/libtorch 多输入]]></title>
        <id>https://txSangyj.github.io/post/torchlibtorch-duo-shu-ru/</id>
        <link href="https://txSangyj.github.io/post/torchlibtorch-duo-shu-ru/">
        </link>
        <updated>2019-08-07T02:40:33.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
一个小坑，多输入的pytorch模型在导出pt文件供libtorch调用时候，python下模型的forward方法不能使用tuple的形式传入inputs。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#pytorch%E4%B8%8B">pytorch下</a>
<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5">数据输入</a></li>
</ul>
</li>
<li><a href="#libtorch%E4%B8%8B">libtorch下</a></li>
</ul>
<br>
一个小坑，多输入的pytorch模型在导出pt文件供libtorch调用时候，python下模型的forward方法不能使用tuple的形式传入inputs。</p>
<!-- more -->
<h1 id="pytorch下">pytorch下</h1>
<p>pytorch下多输入比较方便，修改模型的forward方法就可以。</p>
<h2 id="数据输入">数据输入</h2>
<p>在处理图片时经常需要多输入，比如分类时输入额外特征、检测时输入定界框，而pytorch已经实现了基本的Dataset类，实现多输入使用的就是派生一个自定义的Dataset然后实现数据读取以及__getitem__、__len__方法供Dataloader调用。</p>
<p>下面是自己用到的代码，自定义还是比较简单。</p>
<pre><code class="language-python">
import os
import numpy as np
from torch.utils.data import DataLoader,Dataset
from torchvision.datasets.folder import default_loader


class CustomDataset(Dataset):
    def __init__(self,
                 img_path,
                 txt_path,
                 loader = default_loader,
                 img_transform=None,
                 ):
        with open(txt_path, 'r') as f:
            lines = f.readlines()
            self.imgs = [
                os.path.join(img_path, i.split(',')[0].partition('\\')[2]) for i in lines
            ]
            self.label_list = [i.split('\\')[1] for i in lines]
            self.feature_list = np.array([list(map(float,[i.split(',')[1],i.split(',')[2],
                                            i.split(',')[3],i.split(',')[4],
                                            i.split(',')[5],i.split(',')[6]]))
                                 for i in lines])
        self.feature_list[:,2:] = self.feature_list[:,2:] / 28
        self.img_transform = img_transform
        self.loader = loader
        self.labels = list(set(self.label_list))
        self.labels.sort()
        self.class_to_idx = dict(zip(self.labels ,range(len(self.labels))))
        self.label_list = [self.class_to_idx[c] for c in self.label_list]

    def __getitem__(self, index):
        img_path = self.imgs[index]
        extra_feature = self.feature_list[index]
        label = self.label_list[index]
        img = self.loader(img_path)
        if self.img_transform is not None:
            img = self.img_transform(img)
        return img, extra_feature, label

    def __len__(self):
        return len(self.label_list)

</code></pre>
<h1 id="libtorch下">libtorch下</h1>
<p>libtorch下模型的forward方法的输入是一个向量，如果模型的forward方法每个参数对应一个输入的话，在对应位置输入就没问题，但是如果python模型使用了tuple来传inputs，那可能遇到下面几种参数不匹配的错误，在libtorch中调用模型的forward时无法将输入的std::vector&lt;torch::IValue&gt;转换为(Tensor, Tensor)。</p>
<pre><code>Expected value of type (Tensor, Tensor) for argument 'argument_1' in position 0, but instead got value of type Tensor. Declaration: forward((Tensor, Tensor) argument_1) -&gt; Tensor
</code></pre>
<p>或者</p>
<pre><code>Expected at most 1 argument(s) for operator 'forward', but received 2 argument(s). Declaration: forward((Tensor, Tensor) argument_1) -&gt; Tensor 
</code></pre>
<p>另外还有在torch.jit.trace阶段可能碰到的错误，可以多包裹一层tuple解决，例如((sample_input_1, sample_input_2),)。</p>
<pre><code>TypeError: forward() takes 2 positional arguments but 3 were given</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Pooling]]></title>
        <id>https://txSangyj.github.io/post/universal-pooling/</id>
        <link href="https://txSangyj.github.io/post/universal-pooling/">
        </link>
        <updated>2019-07-30T01:57:38.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
逛Paperweekly看到的一篇论文，只用过池化还没看过相关论文。看完也没有太惊艳的感觉，可能池化操作还是不被看好吧。InceptionV4中的Reduction Block也是Maxpooling和卷积层同时使用，GAN为了不丢失信息也是卷积层替代，除了Global average pooling (GAP)这种替代全连接层的操作。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E6%B1%A0%E5%8C%96">池化</a>
<ul>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%B1%A0%E5%8C%96">常用池化</a></li>
<li><a href="#%E6%B1%A0%E5%8C%96%E4%BD%9C%E7%94%A8">池化作用</a></li>
</ul>
</li>
<li><a href="#universal-pooling">Universal Pooling</a></li>
<li><a href="#%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98">存在的问题</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<br>
逛Paperweekly看到的一篇论文，只用过池化还没看过相关论文。看完也没有太惊艳的感觉，可能池化操作还是不被看好吧。InceptionV4中的Reduction Block也是Maxpooling和卷积层同时使用，GAN为了不丢失信息也是卷积层替代，除了Global average pooling (GAP)这种替代全连接层的操作。</p>
<!-- more -->
<h1 id="池化">池化</h1>
<p>简单的卷积神经网络一般包括卷积层、池化层以及全连接层。池化在其中是一种降采样的过程，主要分为平均池化和最大池化。</p>
<h2 id="常用池化">常用池化</h2>
<p>常见的就是Maxpooling和Meanpooling，容易理解，直接上图。<br>
Maxpooling:<br>
<img src="https://i.loli.net/2019/07/31/5d40f8c5cf32a37790.png" alt="Maxpooling" loading="lazy"><br>
Meanpooling/Averagepooling:<br>
<img src="https://i.loli.net/2019/07/31/5d40f91906e0b64625.png" alt="Meanpooling" loading="lazy"></p>
<h2 id="池化作用">池化作用</h2>
<ul>
<li>降采样，降低参数量和计算量。</li>
<li>增大后续层单元的感受野。</li>
<li>降低微弱噪声和畸变的影响</li>
</ul>
<h1 id="universal-pooling">Universal Pooling</h1>
<p>提出的通用池化层可以生成任意类型的池化，核心类似注意力机制。<br>
<strong>核心</strong>：Universal pooling can be considered as a channel-wise local spatial attention module.</p>
<blockquote>
<p>The basic idea of universal pooling is to interpret pooling as attention and extend it to the general channelwise local spatial attention. That is, the  universal pooling selects pooling weights for each channel and they are <strong>trained</strong> together with other feature extraction parts.</p>
</blockquote>
<p>定义的池化操作的限制：</p>
<ol>
<li>channel-wise:每个channel分开来训练权重参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>。</li>
<li>仅池化给定块</li>
<li>池化不能改变特征的尺度（块内权重之和为1）<br>
<img src="https://i.loli.net/2019/07/30/5d3fda16bc1f829435.jpg" alt="结构" loading="lazy"><br>
结构上Block1.1是一个内部神经网络，还是可以使用卷积层、全连接层、批归一化层等进行构建，神经网络的输入是单个特征图，输出为临时特征图，尺寸和输入相同，经过block1.2进行块内（形状由步长决定）的softmax转化为各分块的内部权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>，然后Pooling map和Feature map作点积、块内求和等操作得到池化后的输出。</li>
</ol>
<h1 id="存在的问题">存在的问题</h1>
<ol>
<li>池化操作使用较少，特征图的尺度变换基本上由卷积层的stride和padding控制。</li>
<li>增加了参数量和计算量，附加了一个内部的神经网络，带来了额外的结构设计问题。</li>
<li>学习MaxPooling这种高度非线性化的函数可能存在困难。</li>
</ol>
<hr>
<h1 id="参考">参考</h1>
<p><a href="https://arxiv.org/abs/1907.11440">论文地址</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[C++ lambda]]></title>
        <id>https://txSangyj.github.io/post/c-lambda/</id>
        <link href="https://txSangyj.github.io/post/c-lambda/">
        </link>
        <updated>2019-07-18T06:49:17.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
C++中lambda的介绍和简单使用。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E6%9E%84%E6%88%90">构成</a>
<ul>
<li><a href="#captures">captures</a></li>
<li><a href="#params">params</a></li>
<li><a href="#ret">ret</a></li>
<li><a href="#body">body</a></li>
</ul>
</li>
<li><a href="#%E4%BE%8B%E5%AD%90">例子</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
<br>
C++中lambda的介绍和简单使用。</p>
<!-- more -->
<h1 id="构成">构成</h1>
<p>C++11下lambda表达式完全体的构成：<br>
[ captures ]  ( params ) specifiers exception attr -&gt; ret { body }</p>
<p>其中params和ret可以省略，省略返回值类型相当于返回auto类型，省略（params）相当于lambda函数不接受参数输入，对应以下几种变体：</p>
<ul>
<li>[ captures ] ( params ) -&gt; ret { body }</li>
<li>[ captures ] ( params ) { body }</li>
<li>[ captures ] { body }</li>
</ul>
<h2 id="captures">captures</h2>
<p>支持值捕获、引用捕获和隐式捕获。<br>
<strong>值捕获和引用捕获</strong><br>
在变量前使用前缀=来表示值捕获，&amp;表示引用捕获。变量在captures列表只能出现一次，当变量被一种方式隐式捕获时，显示捕获只能将其声明为另一种捕获方式。<br>
【1】中的例子：</p>
<pre><code class="language-c++">// 默认引用捕获
struct S2 { void f(int i); };
void S2::f(int i)
{
    [&amp;]{};          // OK: by-reference capture default
    [&amp;, i]{};       // OK: by-reference capture, except i is captured by copy
    [&amp;, &amp;i] {};     // Error: by-reference capture when by-reference is the default
    [&amp;, this] {};   // OK, equivalent to [&amp;]
    [&amp;, this, i]{}; // OK, equivalent to [&amp;, i]
}
</code></pre>
<pre><code class="language-c++">// 默认值捕获
struct S2 { void f(int i); };
void S2::f(int i)
{
    [=]{};          // OK: by-copy capture default
    [=, &amp;i]{};      // OK: by-copy capture, except i is captured by reference
    [=, *this]{};   // until C++17: Error: invalid syntax
                    // since c++17: OK: captures the enclosing S2 by copy
    [=, this] {};   // until C++20: Error: this when = is the default
                    // since C++20: OK, same as [=]
}
</code></pre>
<p><strong>隐式捕获</strong><br>
上面的例子中可以看到，当捕获列表的首项为=及&amp;时会按相应的捕获方式捕获所用的变量和this下的成员变量。另外lambda表达式可以不捕获而使用或读取一些变量，比如全局变量、静态局部变量等，具体类型可以查看参考[1]。</p>
<pre><code class="language-c++">
#include &lt;iostream&gt;
using namespace std;

int i = 10;
int main()
{
    static int j = 1;
    auto f = []() {cout &lt;&lt;&quot;i:&quot;&lt;&lt; i &lt;&lt;&quot;\tj:&quot;&lt;&lt; j; };
    f();
}
// out:
// i:10	j:1
</code></pre>
<h2 id="params">params</h2>
<p>和函数参数差不多，在C++14后不支持指定参数默认值且支持用auto声明参数。<br>
<strong>参数名与变量名相同会隐藏所捕获的变量。</strong></p>
<h2 id="ret">ret</h2>
<p>返回值类型，可省略。省略后要注意不同分支下返回值类型要保持一致。</p>
<h2 id="body">body</h2>
<p>函数体。当变量为值捕获时，要在lambda表达式的parameter list（不可省略） 和 return type（省略或&quot;-&gt;ret&quot;形式）之间指定说明符为mutable才能修改变量的值，但仍是值引用。</p>
<pre><code class="language-c++">
int i = 0;
auto f = [i]() mutable {return ++i; };
cout &lt;&lt; &quot;i:&quot; &lt;&lt; i &lt;&lt; &quot;\tf():&quot; &lt;&lt; f();

// out:
// i:0     f():1
</code></pre>
<h1 id="例子">例子</h1>
<p>在一些需要输入函数的时候就可以用lambda生成匿名函数传入，比如std中的sort、for_each等函数。</p>
<pre><code class="language-c++">int main()
{
    using namespace std;
    vector&lt;int&gt; nums = { 1,2,3,4,5,6,7 };

    // 使用lambda和for_each定义一个函数用于输出vector
    auto print_vec = [](const vector&lt;int&gt; vec) {
        for_each(vec.begin(), vec.end(), [](int i) {cout &lt;&lt; i &lt;&lt; '\t'; });
    };
    print_vec(nums);
    cout &lt;&lt; endl;

    // 引用方式修改值
    for_each(nums.begin(), nums.end(), [](int&amp; i) { i -= 4; });
    print_vec(nums);
    cout &lt;&lt; endl;

    // sort自定义排序
    sort(nums.begin(), nums.end(), [](int a, int b) -&gt; bool { return abs(a) &lt; abs(b); }); 
    print_vec(nums);
    cout &lt;&lt; endl;
}
// out:
// 1       2       3       4       5       6       7
// -3      -2      -1      0       1       2       3
// 0       -1      1       -2      2       -3      3
</code></pre>
<hr>
<h1 id="参考">参考</h1>
<p>[1]<a href="https://zh.cppreference.com/w/cpp/language/lambda">cppreference-lambda</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hexo 转到 Gridea]]></title>
        <id>https://txSangyj.github.io/post/hexo-zhuan-dao-gridea/</id>
        <link href="https://txSangyj.github.io/post/hexo-zhuan-dao-gridea/">
        </link>
        <updated>2019-07-17T06:36:21.000Z</updated>
        <summary type="html"><![CDATA[<p><br>
逛论坛时候发现个新玩具<a href="https://gridea.dev/">Gridea</a>，然后就把原来Hexo下的东西迁到了Gridea下。</p>
]]></summary>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E9%85%8D%E7%BD%AE">配置</a></li>
<li><a href="#%E6%96%87%E7%AB%A0">文章</a></li>
<li><a href="#%E5%9B%BE%E7%89%87">图片</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98">其他问题</a></li>
</ul>
<br>
逛论坛时候发现个新玩具<a href="https://gridea.dev/">Gridea</a>，然后就把原来Hexo下的东西迁到了Gridea下。</p>
<!-- more -->
<h1 id="配置">配置</h1>
<p>上教程<a href="https://gridea.dev/docs/">Gridea doc</a></p>
<h1 id="文章">文章</h1>
<p>原Hexo的source/_post下的.md文件复制到Gridea源文件夹下的posts下。注意posts下<strong>不能</strong>有文件夹。<s>可能是Bug吧。</s></p>
<h1 id="图片">图片</h1>
<p>又不能有asset_folder，Gridea插入的图片使用的又是绝对路径，所以把之前的图片转成了图床链接。用的是<a href="https://github.com/kookob/smpic">Simpic</a>上传。</p>
<h1 id="其他问题">其他问题</h1>
<ul>
<li>原来的主题不会适配。</li>
<li>文章的分类没有自动识别。</li>
<li>Gridea下写东西<strong>千万不要随便点返回</strong>。</li>
<li>建议文章都添加&lt;!-- more --&gt;不然有的主页显示会出现错位。</li>
<li>字数统计、点击统计好像暂不支持。</li>
</ul>
]]></content>
    </entry>
</feed>